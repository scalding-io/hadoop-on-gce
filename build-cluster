#!/usr/bin/env bash

## Leave them empty, set them in file 'cluster.conf'
PROJECT=""
ZONE=""
NODES=""
SSH_PUB_KEY=""
MACHINE_TYPE=""
DISK_SIZE=""

WAIT_BETWEEN_REBOOTS="100"

source cluster.conf

## Two color codes.
SY=$'\x1b[33;1m'
NC=$'\x1b[0m'

## Add a firewall rule to let access to Cloudera Web UI (7180). Rule applies only to VMs with tag cdh.
## Also permit access to 8888 for various Web UIs.
echo "${SY}Adding firewall rule to allow WebUI remote access.${NC}"
[[ $(gcloud compute firewall-rules list --project $PROJECT | grep cloudera-manager) ]] && \
    gcloud compute firewall-rules create cloudera-manager --allow tcp:7180 tcp:8888 tcp:18080 tcp:18088 tcp:8080 tcp:8088 --target-tags cdh --project $PROJECT

## Create nodes.
echo "${SY}Creating VMs${NC}"
echo "${SY}Number of nodes:${NC} $NODES"
echo "${SY}Hard disks' size:${NC} $DISK_SIZE"
echo "" > hosts.txt
for ((i=1; i<=$NODES; i++)); do
    n="$(printf "%02d" $i)"
    gcloud compute instances create node-$n \
        --project $PROJECT \
        --machine-type $MACHINE_TYPE \
        --image centos-6 \
        --zone $ZONE \
        --network default \
        --tags cdh \
        --boot-disk-size $DISK_SIZE
    echo "node-$n" >> hosts.txt
done

ALL_HOSTS=$(cat hosts.txt)
## Wait for the machines to boot
echo "${SY}Waiting for VMs to boot for $WAIT_BETWEEN_REBOOTS seconds.${NC}"
sleep $WAIT_BETWEEN_REBOOTS

## Create a ssh key to use for communication between hosts.
SSH_TEMPFILE="$(mktemp)"
ssh-keygen -n "root@cluster" -N "" -f "${SSH_TEMPFILE}.key"

## Install custom ssh keys, dnsutils, resize the root partition and reboot for the kernel to re-read the partition table, set swapiness, disable strict host checking between our nodes.
echo "${SY}Resizing partitions, adding ssh keys, installing needed packages and rebooting VMs.${NC}"
for ((i=1; i<=$NODES; i++)); do
    n="$(printf "%02d" $i)"

    # These commands disable tty requisite for sudo for centos-6 and redhat distributions. Also it permits root login â€”due to the nature of our setup it is ok.
    gcloud compute ssh node-$n --zone $ZONE --project $PROJECT --ssh-flag="-tt" --command "sudo sed -e 's/\(Defaults.*requiretty\)/#\1/' -i /etc/sudoers"
    gcloud compute ssh node-$n --zone $ZONE --project $PROJECT --command "\
sudo sed -e 's/PermitRootLogin no/PermitRootLogin yes/' -i /etc/ssh/sshd_config; \
sudo service sshd restart; \
sudo mkdir /root/.ssh/;"

    # The ssh public and private keys installed here are to be used by users who are already ssh'd into one of the nodes.
    echo "Teeing SSH keys"
    cat "${SSH_TEMPFILE}.key" | gcloud compute ssh node-$n --zone $ZONE --project $PROJECT --command "sudo tee -a /root/.ssh/id_rsa; sudo chmod 600 /root/.ssh/id_rsa"
    cat "${SSH_TEMPFILE}.key.pub" | gcloud compute ssh node-$n --zone $ZONE --project $PROJECT --command "sudo tee -a /root/.ssh/authorized_keys"

    # The ssh public key installed in this command is to be used by Cloudera Manager
    gcloud compute ssh node-$n --zone $ZONE --project $PROJECT --command "\
echo $SSH_PUB_KEY | sudo tee -a /root/.ssh/authorized_keys; \
echo -e 'Host node-*\n  StrictHostKeyChecking no' | sudo tee -a /root/.ssh/config; \
sudo yum install -y http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm; \
sudo yum install -y htop nload pssh iftop; \
echo 'vm.swappiness = 0' | sudo tee -a /etc/sysctl.conf; \
sudo sed -e 's/SELINUX=enforcing/SELINUX=disabled/' -i /etc/sysconfig/selinux /etc/selinux/config; \
echo -e 'd\nn\np\n1\n17\n\nw' | sudo /sbin/fdisk /dev/sda; \
sudo /sbin/reboot;"
done

# Delete ssh key file.
rm "${SSH_TEMPFILE}" "${SSH_TEMPFILE}.key" "${SSH_TEMPFILE}.key.pub"

## Wait for machines to boot.
echo "${SY}Waiting for VMs to reboot for $WAIT_BETWEEN_REBOOTS seconds.${NC}"
sleep $WAIT_BETWEEN_REBOOTS

## Resize filesystems to utilize all free space.
echo "${SY}Resizing filesystems now that the kernels read the new partition tables.${NC}"
for ((i=1; i<=$NODES; i++)); do
    n="$(printf "%02d" $i)"
    gcloud compute ssh node-$n --zone $ZONE --project $PROJECT --command "sudo /sbin/resize2fs /dev/sda1"
done

## Install cloudera manager on node-01.
echo "${SY}Installing cloudera manager on node-01${NC}"
gcloud compute ssh node-01 --zone $ZONE --project $PROJECT --command "sudo wget http://archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.bin && sudo chmod +x ./cloudera-manager-installer.bin"
gcloud compute copy-files hosts.txt node-01:~ --zone $ZONE --project $PROJECT
gcloud compute copy-files scripts/* node-01:~ --zone $ZONE --project $PROJECT

echo "${SY}Please run ${NC}sudo ./cloudera-manager-installer.bin${SY} to install cloudera-manager.${NC}"
MASTER_IP="$(gcloud compute instances list --project $PROJECT | grep node-01 | awk '{print $5}')"
echo "${SY}When finished, give it a couple minutes to start and visit${NC} http://$MASTER_IP:7180 ${SY}to install the cluster.${NC}"
echo

## Ssh to node-01. User will have to run cloudera manager installer manually the first time.
gcloud compute ssh node-01 --zone $ZONE --project $PROJECT
